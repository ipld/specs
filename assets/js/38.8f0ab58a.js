(window.webpackJsonp=window.webpackJsonp||[]).push([[38],{395:function(e,t,a){"use strict";a.r(t);var o=a(42),n=Object(o.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"specification-hashmap"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#specification-hashmap"}},[e._v("#")]),e._v(" Specification: HashMap")]),e._v(" "),a("p",[a("strong",[e._v("Status: Prescriptive - Draft")])]),e._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"#Introduction"}},[e._v("Introduction")])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Useful-references"}},[e._v("Useful references")])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Summary"}},[e._v("Summary")])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Structure"}},[e._v("Structure")]),e._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"#Parameters"}},[e._v("Parameters")])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Node-properties"}},[e._v("Node properties")])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Schema"}},[e._v("Schema")])])])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Algorithm-in-detail"}},[e._v("Algorithm in detail")]),e._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"#Getkey"}},[a("code",[e._v("Get(key)")])])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Setkey-value"}},[a("code",[e._v("Set(key, value)")])])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Deletekey"}},[a("code",[e._v("Delete(key)")])])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Keys-Values-and-Entries"}},[a("code",[e._v("Keys()")]),e._v(", "),a("code",[e._v("Values()")]),e._v(" and "),a("code",[e._v("Entries()")])])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Differences-to-CHAMP"}},[e._v("Differences to CHAMP")])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Canonical-form"}},[e._v("Canonical form")])])])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Use-as-a-%22Set%22"}},[e._v('Use as a "Set"')])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Implementation-defaults"}},[e._v("Implementation defaults")]),e._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"#hashAlg"}},[a("code",[e._v("hashAlg")])])]),e._v(" "),a("li",[a("a",{attrs:{href:"#bitWidth"}},[a("code",[e._v("bitWidth")])])]),e._v(" "),a("li",[a("a",{attrs:{href:"#bucketSize"}},[a("code",[e._v("bucketSize")])])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Maximum-key-size"}},[e._v("Maximum key size")])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Inline-values"}},[e._v("Inline values")])])])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Possible-future-improvements-and-areas-for-research"}},[e._v("Possible future improvements and areas for research")]),e._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"#Maximum-depth-limitations"}},[e._v("Maximum depth limitations")])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Hash-algorithm"}},[e._v("Hash algorithm")])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Buckets"}},[e._v("Buckets")])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Security"}},[e._v("Security")])])])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Appendix-Filecoin-hamt-variant"}},[e._v("Appendix: Filecoin HAMT Variant")]),e._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"#Implicit-and-fixed-parameters"}},[e._v("Implicit and fixed parameters")])]),e._v(" "),a("li",[a("a",{attrs:{href:"#Block-layout"}},[e._v("Block layout")])])])])]),e._v(" "),a("h2",{attrs:{id:"introduction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),a("p",[e._v("The IPLD HashMap provides multi-block key/value storage and implements the Map "),a("RouterLink",{attrs:{to:"/data-model-layer/data-model.html#kinds"}},[e._v("kind")]),e._v(" as an advanced data layout in the IPLD type system.")],1),e._v(" "),a("p",[e._v("The IPLD HashMap is constructed as a "),a("a",{attrs:{href:"https://en.wikipedia.org/wiki/Hash_array_mapped_trie",target:"_blank",rel:"noopener noreferrer"}},[e._v("hash array mapped trie (HAMT)"),a("OutboundLink")],1),e._v(" with buckets for value storage and "),a("a",{attrs:{href:"https://michael.steindorfer.name/publications/oopsla15.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("CHAMP"),a("OutboundLink")],1),e._v(" mutation semantics. The CHAMP invariant and mutation rules provide us with the ability to maintain canonical forms given any set of keys and their values, regardless of insertion order and intermediate data insertion and deletion. Therefore, for any given set of keys and their values, a consistent IPLD HashMap configuration and block encoding, the root node should always produce the same content identifier (CID).")]),e._v(" "),a("h2",{attrs:{id:"useful-references"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#useful-references"}},[e._v("#")]),e._v(" Useful references")]),e._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://infoscience.epfl.ch/record/64394/files/triesearches.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Fast And Space Efficient Trie Searches"),a("OutboundLink")],1),e._v(" by Phil Bagwell, 2000, and "),a("a",{attrs:{href:"http://lampwww.epfl.ch/papers/idealhashtrees.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ideal Hash Trees"),a("OutboundLink")],1),e._v(" by Phil Bagwell, 2001, introduce the AMT and HAMT concepts.")]),e._v(" "),a("li",[a("a",{attrs:{href:"https://michael.steindorfer.name/publications/oopsla15.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("CHAMP paper"),a("OutboundLink")],1),e._v(" presented at Oopsla 2015 by Steinforder & Vinju")]),e._v(" "),a("li",[a("a",{attrs:{href:"https://github.com/msteindorfer/oopsla15-artifact/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Java implementation"),a("OutboundLink")],1),e._v(" accompanying the original CHAMP paper (see https://github.com/msteindorfer/oopsla15-artifact/blob/master/pdb.values/src/org/eclipse/imp/pdb/facts/util/TrieMap_5Bits.java and other TrieMap files in the same directory).")]),e._v(" "),a("li",[a("a",{attrs:{href:"https://blog.acolyer.org/2015/11/27/hamt/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Optimizing Hash-Array Mapped Tries for Fast and Lean Immutable JVM Collections"),a("OutboundLink")],1),e._v(" a high-level description of HAMT data structures in general and the specifics of CHAMP.")]),e._v(" "),a("li",[e._v("Peergos "),a("a",{attrs:{href:"https://github.com/Peergos/Peergos/blob/master/src/peergos/shared/hamt/Champ.java",target:"_blank",rel:"noopener noreferrer"}},[e._v("CHAMP"),a("OutboundLink")],1),e._v(" implementation")]),e._v(" "),a("li",[a("a",{attrs:{href:"https://github.com/rvagg/iamap",target:"_blank",rel:"noopener noreferrer"}},[e._v("IAMap"),a("OutboundLink")],1),e._v(" JavaScript implementation of the algorithm")]),e._v(" "),a("li",[a("a",{attrs:{href:"https://github.com/rvagg/js-ipld-hashmap",target:"_blank",rel:"noopener noreferrer"}},[e._v("ipld-hashmap"),a("OutboundLink")],1),e._v(" JavaScript IPLD frontend to IAMap with a mutable API")]),e._v(" "),a("li",[a("a",{attrs:{href:"https://github.com/ipfs/go-hamt-ipld",target:"_blank",rel:"noopener noreferrer"}},[e._v("go-hamt-ipld"),a("OutboundLink")],1),e._v(" Filecoin Go HAMT implementation used by the "),a("a",{attrs:{href:"https://lotu.sh/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Lotus"),a("OutboundLink")],1),e._v(" client. See the appendix for how this implementation differs from this specification.")])]),e._v(" "),a("h2",{attrs:{id:"summary"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#summary"}},[e._v("#")]),e._v(" Summary")]),e._v(" "),a("p",[e._v("The HAMT algorithm is used to build the IPLD HashMap. This algorithm is common across many language standard libraries, particularly on the JVM (Clojure, Scala, Java), to power very efficient in-memory unordered key/value storage data structures. We extend the basic algorithm with buckets for block elasticity and strict mutation rules to ensure canonical form.")]),e._v(" "),a("p",[e._v("The HAMT algorithm hashes incoming keys and uses incrementing subsections of that hash at each level of its tree structure to determine the placement of either the entry or a link to a child node of the tree. A "),a("code",[e._v("bitWidth")]),e._v(" determines the number of bits of the hash to use for index calculation at each level of the tree such that the root node takes the first "),a("code",[e._v("bitWidth")]),e._v(" bits of the hash to calculate an index and as we move lower in the tree, we move along the hash by "),a("code",[e._v("depth x bitWidth")]),e._v(" bits. In this way, a sufficiently randomizing hash function will generate a hash that provides a new index at each level of the data structure. An index comprising "),a("code",[e._v("bitWidth")]),e._v(" bits will generate index values of  "),a("code",[e._v("[ 0, 2")]),a("sup",[a("code",[e._v("bitWidth")])]),a("code",[e._v(")")]),e._v(". So a "),a("code",[e._v("bitWidth")]),e._v(" of "),a("code",[e._v("8")]),e._v(" will generate indexes of "),a("code",[e._v("0")]),e._v(" to "),a("code",[e._v("255")]),e._v(" inclusive.")]),e._v(" "),a("p",[e._v("Each node in the tree can therefore hold up to "),a("code",[e._v("2")]),a("sup",[a("code",[e._v("bitWidth")])]),e._v(" elements of data, which we store in an array. In the IPLD HashMap we store entries in buckets. A "),a("code",[e._v("Set(key, value)")]),e._v(" mutation where the index generated at the root node for the hash of "),a("code",[e._v("key")]),e._v(" denotes an array index that does not yet contain an entry, we create a new bucket and insert the "),a("code",[e._v("key")]),e._v(" / "),a("code",[e._v("value")]),e._v(" pair entry. In this way, a single node can theoretically hold up to "),a("code",[e._v("2")]),a("sup",[a("code",[e._v("bitWidth")])]),a("code",[e._v("x bucketSize")]),e._v(" entries, where "),a("code",[e._v("bucketSize")]),e._v(' is the maximum number of elements a bucket is allowed to contain ("collisions"). In practice, indexes do not distribute with perfect randomness so this maximum is theoretical. Entries stored in the node\'s buckets are stored in '),a("code",[e._v("key")]),e._v("-sorted order.")]),e._v(" "),a("p",[e._v("If a "),a("code",[e._v("Set(key, value)")]),e._v(" mutation places a new entry in a bucket that already contains "),a("code",[e._v("bucketSize")]),e._v(" entries, we overflow to a new child node. A new empty node is created and all existing entries in the bucket, in addition to the new "),a("code",[e._v("key")]),e._v(" / "),a("code",[e._v("value")]),e._v(" pair entry are inserted into this new node. We increment the "),a("code",[e._v("depth")]),e._v(" for calculation of the "),a("code",[e._v("index")]),e._v(" from each "),a("code",[e._v("key")]),e._v("'s hash value to calculate the position in the new node's data array. By incrementing "),a("code",[e._v("depth")]),e._v(" we move along by "),a("code",[e._v("bitWidth")]),e._v(" bits in each "),a("code",[e._v("key")]),e._v("'s hash. With a sufficiently random hash function each "),a("code",[e._v("key")]),e._v(" that generated the same "),a("code",[e._v("index")]),e._v(" at a previous level should be distributed roughly evenly in the new node's data array, resulting in a node that contains up to "),a("code",[e._v("bucketSize")]),e._v(" new buckets.")]),e._v(" "),a("p",[e._v("The process of generating "),a("code",[e._v("index")]),e._v(" values from "),a("code",[e._v("bitWidth")]),e._v(" subsections of the hash values provides us with a depth of up to "),a("code",[e._v("(digestLength x 8) / bitWidth")]),e._v(" levels in our tree data structure where "),a("code",[e._v("digestLength")]),e._v(" is the number of output bytes generated by the hash function. With each node able to store up to "),a("code",[e._v("2")]),a("sup",[a("code",[e._v("bitWidth")])]),e._v(" child node references and up to "),a("code",[e._v("bucketSize")]),e._v(" elements able to be stored in colliding leaf positions we are able to store a very large number of entries. A hash function's randomness will dictate the  even distribution of elements and a hash function's output "),a("code",[e._v("digestLength")]),e._v(" will dictate the maximum depth of the tree.")]),e._v(" "),a("p",[e._v("A further optimization is applied to reduce the storage requirements of HAMT nodes. The data elements array is only allocated to be long enough to store actual entries: non-empty buckets or links to actual child nodes. An empty or "),a("code",[e._v("Null")]),e._v(" array index is not used as a signal that a "),a("code",[e._v("key")]),e._v(" does not exist in that node. Instead, the data elements array is compacted by use of a "),a("code",[e._v("map")]),e._v(" bitfield where each bit of "),a("code",[e._v("map")]),e._v(" corresponds to an "),a("code",[e._v("index")]),e._v(" in the node. When an "),a("code",[e._v("index")]),e._v(" is generated, the "),a("code",[e._v("index")]),e._v(" bit of the "),a("code",[e._v("map")]),e._v(" bitfield is checked. If the bit is not set ("),a("code",[e._v("0")]),e._v("), that index does not exist. If the bit is set ("),a("code",[e._v("1")]),e._v("), the value exists in the data elements array. To determine the index of the data elements array, we perform a bit-count ("),a("code",[e._v("popcount()")]),e._v(") on the "),a("code",[e._v("map")]),e._v(" bitfield "),a("em",[e._v("up to")]),e._v(" the "),a("code",[e._v("index")]),e._v(" bit to generate a "),a("code",[e._v("dataIndex")]),e._v(". In this way, the data elements array's total length is equal to "),a("code",[e._v("popcount(map)")]),e._v(" (the number of bits set in all of "),a("code",[e._v("map")]),e._v("). If "),a("code",[e._v("map")]),e._v("'s bits are all set then the data elements array will be "),a("code",[e._v("2")]),a("sup",[a("code",[e._v("bitWidth")])]),e._v(" in length, i.e. every position will contain either a bucket or a link to a child node.")]),e._v(" "),a("p",[e._v("Insertion of new buckets with "),a("code",[e._v("Set(key, value)")]),e._v(" involves splicing in a new element to the data array at the "),a("code",[e._v("dataIndex")]),e._v(" position and setting the "),a("code",[e._v("index")]),e._v(" bit of the "),a("code",[e._v("map")]),e._v(" bitmap. Converting a bucket to a child node leaves the "),a("code",[e._v("map")]),e._v(" bit map alone as the "),a("code",[e._v("index")]),e._v(" bit still indicates there is an element at that position.")]),e._v(" "),a("p",[e._v("A "),a("code",[e._v("Get(key)")]),e._v(" operation performs the same hash, "),a("code",[e._v("index")]),e._v(" and "),a("code",[e._v("dataIndex")]),e._v(" calculation at the root node, traversing into a bucket to find an entry matching "),a("code",[e._v("key")]),e._v(" or traversing into child nodes and performing the same "),a("code",[e._v("index")]),e._v(" and "),a("code",[e._v("dataIndex")]),e._v(" calculation but at an offset of an additional "),a("code",[e._v("bitWidth")]),e._v(" bits in the "),a("code",[e._v("key")]),e._v("'s hash.")]),e._v(" "),a("p",[e._v("A "),a("code",[e._v("Delete(key)")]),e._v(" mutation first locates the element in the same way as "),a("code",[e._v("Get(key)")]),e._v(" and if that entry exists, it is removed from the bucket containing it. If the bucket is empty after deletion of the entry, we remove the bucket element completely from the data element array and unsets the "),a("code",[e._v("index")]),e._v(" bit of "),a("code",[e._v("map")]),e._v(". If the node containing the deleted element has no links to child nodes and contains "),a("code",[e._v("bucketSize")]),e._v(" elements after the deletion, those elements are compacted into a single bucket and placed in the parent node in place of the link to that node. We perform this check on the parent (and recursively if required), thereby transforming the tree into its most compact form, with only buckets in place of nodes that have up to "),a("code",[e._v("bucketSize")]),e._v(" entries at all edges. This compaction process combined with the "),a("code",[e._v("key")]),e._v(" ordering of entries in buckets produces canonical forms of the data structure for any given set of "),a("code",[e._v("key")]),e._v(" / "),a("code",[e._v("value")]),e._v(" pairs regardless of their insertion order or whether any intermediate entries have been added and deleted.")]),e._v(" "),a("p",[e._v("By default, each node in an IPLD HashMap is stored in a distinct IPLD block and CIDs are used for child node links. The schema and algorithm presented here also allows for inline child nodes rather than links, with read operations able to traverse multiple nodes within a single block where they are inlined. The production of inlined IPLD HashMaps is left unspecified and users should be aware that inlining breaks canonical form guarantees.")]),e._v(" "),a("h2",{attrs:{id:"structure"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#structure"}},[e._v("#")]),e._v(" Structure")]),e._v(" "),a("h3",{attrs:{id:"parameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameters"}},[e._v("#")]),e._v(" Parameters")]),e._v(" "),a("p",[e._v("Configurable parameters for any given IPLD HashMap:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("hashAlg")]),e._v(": The hash algorithm applied to keys in order to evenly distribute entries throughout the data structure. The algorithm is chosen based on speed, "),a("code",[e._v("digestLength")]),e._v(" and randomness properties (but it must be available to the reader, hence the need for shared defaults, see below).")]),e._v(" "),a("li",[a("code",[e._v("bitWidth")]),e._v(": The number of bits to use at each level of the data structure for determining the index of the entry or a link to the next level of the data structure to continue searching. The equation "),a("code",[e._v("2")]),a("sup",[a("code",[e._v("bitWidth")])]),e._v(" yields the arity of the HashMap nodes, i.e. the number of storage locations for buckets and/or links to child nodes.")]),e._v(" "),a("li",[a("code",[e._v("bucketSize")]),e._v(": The maximum array size of entry storage buckets such that exceeding "),a("code",[e._v("bucketSize")]),e._v(" causes the creation of a new child node to replace entry storage.")])]),e._v(" "),a("h3",{attrs:{id:"node-properties"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#node-properties"}},[e._v("#")]),e._v(" Node properties")]),e._v(" "),a("p",[e._v("Each node in a HashMap data structure contains:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("data")]),e._v(": An Array, with a length of one to "),a("code",[e._v("2")]),a("sup",[a("code",[e._v("bitWidth")])]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("map")]),e._v(": A bitfield, stored as Bytes, where the first "),a("code",[e._v("2")]),a("sup",[a("code",[e._v("bitWidth")])]),e._v(" bits are used to indicate whether a bucket or child node link is present at each possible index of the node.")])]),e._v(" "),a("p",[e._v("An important property of a HAMT is that the "),a("code",[e._v("data")]),e._v(" array only contains active elements. Indexes in a node that do not contain any values (in buckets or links to child nodes) are not stored and the "),a("code",[e._v("map")]),e._v(" bitfield is used to determine the "),a("code",[e._v("data")]),e._v(" whether values are present and the array index of present values using a "),a("a",{attrs:{href:"https://en.wikipedia.org/wiki/Hamming_weight",target:"_blank",rel:"noopener noreferrer"}},[a("code",[e._v("popcount()")]),a("OutboundLink")],1),e._v(". This allows us to store a maximally compacted "),a("code",[e._v("data")]),e._v(" array for each node.")]),e._v(" "),a("h3",{attrs:{id:"schema"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#schema"}},[e._v("#")]),e._v(" Schema")]),e._v(" "),a("p",[e._v("The "),a("strong",[e._v("root block")]),e._v(" of an IPLD HashMap contains the same properties as all other blocks, in addition to configuration data that dictates how the algorithm below traverses and mutates the data structure.")]),e._v(" "),a("p",[e._v("See "),a("a",{attrs:{href:"../../schemas"}},[e._v("IPLD Schemas")]),e._v(" for a definition of this format.")]),e._v(" "),a("div",{staticClass:"language-ipldsch extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ipldsch"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# Root node layout")]),e._v("\n"),a("span",{pre:!0,attrs:{class:"token typedef"}},[a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("type")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("HashMapRoot")])]),e._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[e._v("struct")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("\n  hashAlg Int\n  bucketSize Int\n  hamt HashMapNode\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# Non-root node layout")]),e._v("\n"),a("span",{pre:!0,attrs:{class:"token typedef"}},[a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("type")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("HashMapNode")])]),e._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[e._v("struct")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("\n  map Bytes\n  data "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("[")]),e._v(" Element "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("]")]),e._v("\n"),a("span",{pre:!0,attrs:{class:"token representation"}},[e._v("} "),a("span",{pre:!0,attrs:{class:"token builtin"}},[e._v("representation")])]),e._v(" tuple\n\n"),a("span",{pre:!0,attrs:{class:"token typedef"}},[a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("type")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("Element")])]),e._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[e._v("union")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("|")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("&")]),e._v("HashMapNode link\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("|")]),e._v(" Bucket list\n"),a("span",{pre:!0,attrs:{class:"token representation"}},[e._v("} "),a("span",{pre:!0,attrs:{class:"token builtin"}},[e._v("representation")])]),e._v(" kinded\n\n"),a("span",{pre:!0,attrs:{class:"token typedef"}},[a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("type")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("Bucket")])]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("[")]),e._v(" BucketEntry "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("]")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token typedef"}},[a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("type")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("BucketEntry")])]),e._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[e._v("struct")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("\n  key Bytes\n  value Any\n"),a("span",{pre:!0,attrs:{class:"token representation"}},[e._v("} "),a("span",{pre:!0,attrs:{class:"token builtin"}},[e._v("representation")])]),e._v(" tuple\n")])])]),a("p",[e._v("Notes:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("hashAlg")]),e._v(" in the root block is an integer code identifying the hash algorithm used for mapping keys to their positions in the structure. The code should correspond to a "),a("a",{attrs:{href:"https://github.com/multiformats/multihash",target:"_blank",rel:"noopener noreferrer"}},[e._v("multihash"),a("OutboundLink")],1),e._v(" code as found in the "),a("a",{attrs:{href:"https://github.com/multiformats/multicodec/blob/master/table.csv",target:"_blank",rel:"noopener noreferrer"}},[e._v("multiformats table"),a("OutboundLink")],1),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("bitWidth")]),e._v(" in the root block must be at least "),a("code",[e._v("3")]),e._v(", making the minimum "),a("code",[e._v("map")]),e._v(" size 1 byte.")]),e._v(" "),a("li",[a("code",[e._v("bitWidth")]),e._v(" is not present in the root block as it is inferred from the size of the "),a("code",[e._v("map")]),e._v(" byte array with the equation "),a("code",[e._v("log2(byteLength(map) x 8)")]),e._v(", being the inverse of the "),a("code",[e._v("map")]),e._v(" size equation "),a("code",[e._v("2")]),a("sup",[a("code",[e._v("bitWidth")])]),a("code",[e._v("/ 8")]),e._v(". For lower-level languages, you can use the cheaper equivalent "),a("code",[e._v("trailingZeroBits(byteLength(map)) + 3")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("bucketSize")]),e._v(" in the root block must be at least "),a("code",[e._v("1")]),e._v(".")]),e._v(" "),a("li",[e._v("Keys are stored in "),a("code",[e._v("Byte")]),e._v(" form.")]),e._v(" "),a("li",[a("code",[e._v("Element")]),e._v(" is a kinded union that supports storing either a "),a("code",[e._v("Bucket")]),e._v(" (as kind "),a("code",[e._v("list")]),e._v(") or a link to a child node (as kind "),a("code",[e._v("link")]),e._v(").")])]),e._v(" "),a("h2",{attrs:{id:"algorithm-in-detail"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#algorithm-in-detail"}},[e._v("#")]),e._v(" Algorithm in detail")]),e._v(" "),a("h3",{attrs:{id:"get-key"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-key"}},[e._v("#")]),e._v(" "),a("code",[e._v("Get(key)")])]),e._v(" "),a("ol",[a("li",[e._v("Set a "),a("code",[e._v("depth")]),e._v(" value to "),a("code",[e._v("0")]),e._v(", indicating the root block")]),e._v(" "),a("li",[e._v("The "),a("code",[e._v("key")]),e._v(" is hashed, using "),a("code",[e._v("hashAlg")]),e._v(".")]),e._v(" "),a("li",[e._v("Take the left-most "),a("code",[e._v("bitWidth")]),e._v(" bits, offset by "),a("code",[e._v("depth x bitWidth")]),e._v(", from the hash to form an "),a("code",[e._v("index")]),e._v(". At each level of the data structure, we increment the section of bits we take from the hash so that the "),a("code",[e._v("index")]),e._v(" comprises a different set of bits as we move down.")]),e._v(" "),a("li",[e._v("If the "),a("code",[e._v("index")]),e._v(" bit in the node's "),a("code",[e._v("map")]),e._v(" is "),a("code",[e._v("0")]),e._v(", we can be certain that the "),a("code",[e._v("key")]),e._v(" does not exist in this data structure, so return an empty value (as appropriate for the implementation platform).")]),e._v(" "),a("li",[e._v("If the "),a("code",[e._v("index")]),e._v(" bit in the node's "),a("code",[e._v("map")]),e._v(" is "),a("code",[e._v("1")]),e._v(", the value may exist. Perform a "),a("code",[e._v("popcount()")]),e._v(" on the "),a("code",[e._v("map")]),e._v(" up to "),a("code",[e._v("index")]),e._v(" such that we count the number of "),a("code",[e._v("1")]),e._v(" bits up to the "),a("code",[e._v("index")]),e._v(" bit-position. This gives us "),a("code",[e._v("dataIndex")]),e._v(", an index in the "),a("code",[e._v("data")]),e._v(" array to look up the value or insert a new bucket.")]),e._v(" "),a("li",[e._v("If the "),a("code",[e._v("dataIndex")]),e._v(" element of "),a("code",[e._v("data")]),e._v(" contains a link (CID) to a child block, increment "),a("code",[e._v("depth")]),e._v(" and repeat with the child node identified by the link from step "),a("strong",[e._v("3")]),e._v(".")]),e._v(" "),a("li",[e._v("If the "),a("code",[e._v("dataIndex")]),e._v(" element of "),a("code",[e._v("data")]),e._v(" contains a bucket (array), iterate through entries in the bucket:\n"),a("ol",[a("li",[e._v("If an entry has the "),a("code",[e._v("key")]),e._v(" we are looking for, return the "),a("code",[e._v("value")]),e._v(".")]),e._v(" "),a("li",[e._v("If no entries contain the "),a("code",[e._v("key")]),e._v(" we are looking for, return an empty value (as appropriate for the implementation platform). Note that the bucket will be sorted by "),a("code",[e._v("key")]),e._v(" so a scan can stop when a scan yields keys greater than "),a("code",[e._v("key")]),e._v(".")])])])]),e._v(" "),a("h3",{attrs:{id:"set-key-value"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-key-value"}},[e._v("#")]),e._v(" "),a("code",[e._v("Set(key, value)")])]),e._v(" "),a("ol",[a("li",[e._v("Set a "),a("code",[e._v("depth")]),e._v(" value to "),a("code",[e._v("0")]),e._v(", indicating the root block")]),e._v(" "),a("li",[e._v("The "),a("code",[e._v("key")]),e._v(" is hashed, using "),a("code",[e._v("hashAlg")]),e._v(".")]),e._v(" "),a("li",[e._v("Take the left-most "),a("code",[e._v("bitWidth")]),e._v(" bits, offset by "),a("code",[e._v("depth x bitWidth")]),e._v(", from the hash to form an "),a("code",[e._v("index")]),e._v(". At each level of the data structure, we increment the section of bits we take from the hash so that the "),a("code",[e._v("index")]),e._v(" comprises a different set of bits as we move down.")]),e._v(" "),a("li",[e._v("If the "),a("code",[e._v("index")]),e._v(" bit in the node's "),a("code",[e._v("map")]),e._v(" is "),a("code",[e._v("0")]),e._v(", a new bucket needs to be created at the current node. If the "),a("code",[e._v("index")]),e._v(" bit in the node's "),a("code",[e._v("map")]),e._v(" is "),a("code",[e._v("1")]),e._v(", a value exists for this "),a("code",[e._v("index")]),e._v(" in the node's "),a("code",[e._v("data")]),e._v(" which may be a bucket (which may be full) or may be a link to a child node.")]),e._v(" "),a("li",[e._v("Perform a "),a("code",[e._v("popcount()")]),e._v(" on the "),a("code",[e._v("map")]),e._v(" up to "),a("code",[e._v("index")]),e._v(" such that we count the number of "),a("code",[e._v("1")]),e._v(" bits up to the "),a("code",[e._v("index")]),e._v(" bit-position. This gives us "),a("code",[e._v("dataIndex")]),e._v(", an index in the "),a("code",[e._v("data")]),e._v(" array to look up the value or insert a new bucket.")]),e._v(" "),a("li",[e._v("If the "),a("code",[e._v("index")]),e._v(" bit in the node's "),a("code",[e._v("map")]),e._v(" is "),a("code",[e._v("0")]),e._v(":\n"),a("ol",[a("li",[e._v("Mutate the current node (create a copy).")]),e._v(" "),a("li",[e._v("Insert a new element in "),a("code",[e._v("data")]),e._v(" at "),a("code",[e._v("dataIndex")]),e._v(" containing an new bucket (array) with a single entry for the "),a("code",[e._v("key")]),e._v(" / "),a("code",[e._v("value")]),e._v(" pair.")]),e._v(" "),a("li",[e._v("Create a CID for the mutated node.")]),e._v(" "),a("li",[e._v("If "),a("code",[e._v("depth")]),e._v(" is "),a("code",[e._v("0")]),e._v(", the CID represents the new root block of the HashMap.")]),e._v(" "),a("li",[e._v("If "),a("code",[e._v("depth")]),e._v(" is greater than "),a("code",[e._v("0")]),e._v(":\n"),a("ol",[a("li",[e._v("Mutate the node's parent")]),e._v(" "),a("li",[e._v("Record the new CID of the mutated child in the appropriate position of the mutated parent's "),a("code",[e._v("data")]),e._v(" array.")]),e._v(" "),a("li",[e._v("Recursively proceed, by recording the new CIDs of each node in a mutated copy of its parent node until "),a("code",[e._v("depth")]),e._v(" of "),a("code",[e._v("0")]),e._v(" where we produce the the new root block and its CID.")])])])])]),e._v(" "),a("li",[e._v("If the "),a("code",[e._v("index")]),e._v(" bit in the node's "),a("code",[e._v("map")]),e._v(" is "),a("code",[e._v("1")]),e._v(":\n"),a("ol",[a("li",[e._v("If the "),a("code",[e._v("dataIndex")]),e._v(" element of "),a("code",[e._v("data")]),e._v(" contains a link (CID) to a child node, increment "),a("code",[e._v("depth")]),e._v(":\n"),a("ol",[a("li",[e._v("If "),a("code",[e._v("(depth x bitWidth) / 8")]),e._v(" is now greater than the "),a("code",[e._v("digestLength")]),e._v(', a "max collisions" failure has occurred and an error state should be returned to the user.')]),e._v(" "),a("li",[e._v("If "),a("code",[e._v("(depth x bitWidth) / 8")]),e._v(" is less than the number of bytes in the hash, repeat with the child node identified in step "),a("strong",[e._v("3")]),e._v(".")])])]),e._v(" "),a("li",[e._v("If the "),a("code",[e._v("dataIndex")]),e._v(" element of "),a("code",[e._v("data")]),e._v(" contains a bucket (array) and the bucket's size is less than "),a("code",[e._v("bucketSize")]),e._v(":\n"),a("ol",[a("li",[e._v("Mutate the current node (create a copy).")]),e._v(" "),a("li",[e._v("If "),a("code",[e._v("key")]),e._v(" is already present in the new bucket, change its corresponding "),a("code",[e._v("value")]),e._v(" to the new one. Otherwise, insert the "),a("code",[e._v("key")]),e._v(" / "),a("code",[e._v("value")]),e._v(" pair at a position sorted by "),a("code",[e._v("key")]),e._v(" such that all entries in the bucket are ordered respective to their "),a("code",[e._v("key")]),e._v("s. This helps ensure canonical form.")]),e._v(" "),a("li",[e._v("Proceed to create new CIDs for the current block and each parent as per step "),a("strong",[e._v("6.c")]),e._v(". until we have a new root block and its CID.")])])]),e._v(" "),a("li",[e._v("If the "),a("code",[e._v("dataIndex")]),e._v(" element of "),a("code",[e._v("data")]),e._v(" contains a bucket (array) and the bucket's size is "),a("code",[e._v("bucketSize")]),e._v(":\n"),a("ol",[a("li",[e._v("Create a new empty node")]),e._v(" "),a("li",[e._v("For each element of the bucket, perform a "),a("code",[e._v("Set(key, value)")]),e._v(" on the new empty node with a "),a("code",[e._v("depth")]),e._v(" set to "),a("code",[e._v("depth + 1")]),e._v(", proceeding from step "),a("strong",[e._v("2")]),e._v(". This should create a new node with "),a("code",[e._v("bucketSize")]),e._v(" elements distributed approximately evenly through its "),a("code",[e._v("data")]),e._v(" array. This operation will only result in more than one new node being created if all "),a("code",[e._v("key")]),e._v("s being set have the same "),a("code",[e._v("bitWidth")]),e._v(" bits of their hashes at "),a("code",[e._v("bitWidth")]),e._v(" position "),a("code",[e._v("depth + 1")]),e._v(" (and so on). A sufficiently random hash algorithm should prevent this from occuring.")]),e._v(" "),a("li",[e._v("Create a CID for the new child node.")]),e._v(" "),a("li",[e._v("Mutate the current node (create a copy)")]),e._v(" "),a("li",[e._v("Replace "),a("code",[e._v("dataIndex")]),e._v(" of "),a("code",[e._v("data")]),e._v(" with a link to the new child node.")]),e._v(" "),a("li",[e._v("Proceed to create new CIDs for the current block and each parent as per step "),a("strong",[e._v("6.c")]),e._v(". until we have a new root block and its CID.")])])])])])]),e._v(" "),a("h3",{attrs:{id:"delete-key"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#delete-key"}},[e._v("#")]),e._v(" "),a("code",[e._v("Delete(key)")])]),e._v(" "),a("p",[e._v('The deletion algorithm below is presented as an iterative operation. It can also be usefully conceived of as a recursive algorithm, which is particularly helpful in the case of node collapsing. See section "4.2 Deletion Algorithm" of the '),a("a",{attrs:{href:"https://michael.steindorfer.name/publications/oopsla15.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("CHAMP paper"),a("OutboundLink")],1),e._v(" for a description of this algorithm. Note that the linked paper does not make use of buckets so note the importance of counting entries in a node and comparing to "),a("code",[e._v("bucketSize")]),e._v(" in the algorithm below.")]),e._v(" "),a("ol",[a("li",[e._v("Set a "),a("code",[e._v("depth")]),e._v(" value to "),a("code",[e._v("0")]),e._v(", indicating the root block")]),e._v(" "),a("li",[e._v("The "),a("code",[e._v("key")]),e._v(" is hashed, using "),a("code",[e._v("hashAlg")]),e._v(".")]),e._v(" "),a("li",[e._v("Take the left-most "),a("code",[e._v("bitWidth")]),e._v(" bits, offset by "),a("code",[e._v("depth x bitWidth")]),e._v(", from the hash to form an "),a("code",[e._v("index")]),e._v(". At each level of the data structure, we increment the section of bits we take from the hash so that the "),a("code",[e._v("index")]),e._v(" comprises a different set of bits as we move down.")]),e._v(" "),a("li",[e._v("If the "),a("code",[e._v("index")]),e._v(" bit in the node's "),a("code",[e._v("map")]),e._v(" is "),a("code",[e._v("0")]),e._v(", we can be certain that the "),a("code",[e._v("key")]),e._v(" does not exist in this data structure, so there is no need to proceed.")]),e._v(" "),a("li",[e._v("If the "),a("code",[e._v("index")]),e._v(" bit in the node's "),a("code",[e._v("map")]),e._v(" is "),a("code",[e._v("1")]),e._v(", the value may exist. Perform a "),a("code",[e._v("popcount()")]),e._v(" on the "),a("code",[e._v("map")]),e._v(" up to "),a("code",[e._v("index")]),e._v(" such that we count the number of "),a("code",[e._v("1")]),e._v(" bits up to the "),a("code",[e._v("index")]),e._v(" bit-position. This gives us "),a("code",[e._v("dataIndex")]),e._v(", an index in the "),a("code",[e._v("data")]),e._v(" array to look up the value or insert a new bucket.")]),e._v(" "),a("li",[e._v("If the "),a("code",[e._v("dataIndex")]),e._v(" element of "),a("code",[e._v("data")]),e._v(" contains a link (CID) to a child node, increment "),a("code",[e._v("depth")]),e._v(" and repeat with the child node identified in step "),a("strong",[e._v("3")]),e._v(".")]),e._v(" "),a("li",[e._v("If the "),a("code",[e._v("dataIndex")]),e._v(" element of "),a("code",[e._v("data")]),e._v(" contains a bucket (array), iterate through entries in the bucket:\n"),a("ol",[a("li",[e._v("If no entries contain the "),a("code",[e._v("key")]),e._v(" we are looking for, there is no need to proceed. Note that the bucket will be sorted by "),a("code",[e._v("key")]),e._v(" so a scan can stop when a scan yields keys greater than "),a("code",[e._v("key")]),e._v(".")]),e._v(" "),a("li",[e._v("If an entry has the "),a("code",[e._v("key")]),e._v(" we are looking for, we need to remove it and possibly collapse this node and any number of parent nodes depending on the number of entries remaining. This helps ensure canonical form. Note that there are two possible states below, if neither case matches the state of the current node, we have not satisfied the invariant and the tree is not in a canonical state (i.e. something has failed):\n"),a("ol",[a("li",[e._v("If "),a("code",[e._v("depth")]),e._v(" is "),a("code",[e._v("0")]),e._v(" (the root node) or there are links in the "),a("code",[e._v("data")]),e._v(" array for this node (it has child nodes) or the number of entries across all buckets in this node is currently greater than "),a("code",[e._v("bucketSize + 1")]),e._v(", we don't need to collapse this node into its parent, but can simply remove the entry from its bucket.\n"),a("ol",[a("li",[e._v("Mutate the current node (create a copy)\n"),a("ol",[a("li",[e._v("If the bucket located at "),a("code",[e._v("dataIndex")]),e._v(" of the "),a("code",[e._v("data")]),e._v(" array contains more than one element, remove the entry from the bucket at "),a("code",[e._v("dataIndex")]),e._v(" of "),a("code",[e._v("data")]),e._v(" (the remaining elements must remain sorted by "),a("code",[e._v("key")]),e._v(").")]),e._v(" "),a("li",[e._v("If the bucket located at "),a("code",[e._v("dataIndex")]),e._v(" of the "),a("code",[e._v("data")]),e._v(" array contains only one element:\n"),a("ol",[a("li",[e._v("Remove "),a("code",[e._v("dataIndex")]),e._v(" of "),a("code",[e._v("data")]),e._v(" (such that "),a("code",[e._v("data")]),e._v(" now has one less element)")]),e._v(" "),a("li",[e._v("Set the "),a("code",[e._v("index")]),e._v(" bit of "),a("code",[e._v("map")]),e._v(" to "),a("code",[e._v("0")])])])]),e._v(" "),a("li",[e._v("Create a CID for the new node with the entry removed.")]),e._v(" "),a("li",[e._v("Record the new CID of the mutated child in the appropriate position of the mutated parent's "),a("code",[e._v("data")]),e._v(" array.")]),e._v(" "),a("li",[e._v("Recursively proceed, by recording the new CIDs of each node in a mutated copy of its parent node until "),a("code",[e._v("depth")]),e._v(" of "),a("code",[e._v("0")]),e._v(" where we produce the the new root block and its CID.")])])])])]),e._v(" "),a("li",[e._v("If "),a("code",[e._v("depth")]),e._v(" is not "),a("code",[e._v("0")]),e._v(" (not the root node) and there are no links in the "),a("code",[e._v("data")]),e._v(" array for this node (it has no child nodes) and the number of entries across all buckets in this node is currently equal to "),a("code",[e._v("bucketSize + 1")]),e._v(", then this node needs to be collapsed into a single bucket, of "),a("code",[e._v("bucketSize")]),e._v(" once the entry being deleted is removed, and replaced in its parent's "),a("code",[e._v("data")]),e._v(" array in place of the link to this node.\n"),a("ol",[a("li",[e._v("Create a new bucket and place all entries in the node except for the one being removed into the new bucket. The new bucket now contains all of the entries from the node and will be used to replace the node in the parent.")]),e._v(" "),a("li",[e._v("Mutate the parent node (create a copy).")]),e._v(" "),a("li",[e._v("Replace the link to the node in the parent's "),a("code",[e._v("data")]),e._v(" array with the newly created bucket. (Note the position in the parent's "),a("code",[e._v("data")]),e._v(" array will be dependendent on the "),a("code",[e._v("key")]),e._v("'s "),a("code",[e._v("index")]),e._v(" at "),a("code",[e._v("depth - 1")]),e._v(" and the "),a("code",[e._v("dataIndex")]),e._v(" calculated from the parent's "),a("code",[e._v("map")]),e._v(").")]),e._v(" "),a("li",[e._v("Create a CID for the new parent.")]),e._v(" "),a("li",[e._v("If the parent is at a "),a("code",[e._v("depth")]),e._v(" of "),a("code",[e._v("0")]),e._v(", i.e. the parent node, the CID represents the new root node.")]),e._v(" "),a("li",[e._v("If the parent is not at "),a("code",[e._v("depth")]),e._v(" of "),a("code",[e._v("0")]),e._v(", repeat from step "),a("strong",[e._v("7.2")]),e._v(" with the parent node. This process should repeat up the tree all the way to "),a("code",[e._v("depth")]),e._v(" of "),a("code",[e._v("0")]),e._v(", potentially collapsing more than one node into its parent in the process.")])])])])])])])]),e._v(" "),a("h3",{attrs:{id:"keys-values-and-entries"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#keys-values-and-entries"}},[e._v("#")]),e._v(" "),a("code",[e._v("Keys()")]),e._v(", "),a("code",[e._v("Values()")]),e._v(" and "),a("code",[e._v("Entries()")])]),e._v(" "),a("p",[e._v("These collection-spanning iteration operations are "),a("strong",[e._v("optional")]),e._v(" for implementations.")]),e._v(" "),a("p",[e._v("The storage order of entries in an IPLD HashMap is entirely dependent on the hash algorithm and "),a("code",[e._v("bitWidth")]),e._v(". Therefore IPLD HashMaps are considered to be random for practical purposes (as opposed to ordered-by-construction or ordered-by-comparator, see "),a("a",{attrs:{href:"https://github.com/ipld/specs/blob/master/data-structures/multiblock-collections.md#collection-types%5D",target:"_blank",rel:"noopener noreferrer"}},[e._v("IPLD Multi-block Collections / Collection types"),a("OutboundLink")],1),e._v("). It is left to the implementation to decide the tree-traversal order and algorithm used to iterate over entries.")]),e._v(" "),a("p",[e._v("An implementation should only emit any given "),a("code",[e._v("key")]),e._v(", "),a("code",[e._v("value")]),e._v(" or "),a("code",[e._v("key")]),e._v(" / "),a("code",[e._v("value")]),e._v(" entry pair once per iteration.")]),e._v(" "),a("h3",{attrs:{id:"differences-to-champ"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#differences-to-champ"}},[e._v("#")]),e._v(" Differences to CHAMP")]),e._v(" "),a("p",[e._v("This algorithm differs from CHAMP in the following ways:")]),e._v(" "),a("ol",[a("li",[e._v("CHAMP separates "),a("code",[e._v("map")]),e._v(" into "),a("code",[e._v("datamap")]),e._v(" and "),a("code",[e._v("nodemap")]),e._v(" for referencing local data elements and local references to child nodes. The "),a("code",[e._v("data")]),e._v(" array is then split in half such that data elements are stored from the left and the child node links are stored from the right with a reverse index. This allows important speed and cache-locality optimizations for fully in-memory data structures but those optimizations are not present, or make negligible impact in a distributed data structure.")]),e._v(" "),a("li",[e._v("CHAMP does not make use, of buckets, nor do common implementations of HAMTs on the JVM (e.g. Clojure, Scala, Java). Storing only entries and links removes the need for an iterative search and compare within buckets and allows direct traversal to the entries required. This is effective for in-memory data structures but is less useful when performing block-by-block traversal with distributed data structures where packing data to reduce traversals may be more important.")])]),e._v(" "),a("h3",{attrs:{id:"canonical-form"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#canonical-form"}},[e._v("#")]),e._v(" Canonical form")]),e._v(" "),a("p",[e._v("To achieve canonical forms for any given set of "),a("code",[e._v("key")]),e._v(" / "),a("code",[e._v("value")]),e._v(" pairs, we note the following properties in the algorithm:")]),e._v(" "),a("ol",[a("li",[e._v("We must keep buckets sorted by "),a("code",[e._v("key")]),e._v(" during both insertion and deletion operations.")]),e._v(" "),a("li",[e._v("We must retain an invariant that states that no non-root node may contain, either directly or via links through child nodes, less than "),a("code",[e._v("bucketSize + 1")]),e._v(" entries. By applying this strictly during the deletion process, we can generalize that no non-root node without links to child nodes may contain less than "),a("code",[e._v("bucketSize + 1")]),e._v(" entries. Any non-root node in the tree breaking this rule during the deletion process must have its entries collapsed into a single bucket of "),a("code",[e._v("bucketSize")]),e._v(" length (i.e. without the entry being removed) and inserted into its parent node in place of the link to the impacted node. We continue to apply this rule recursively up the tree, potentially collapsing additional nodes into their parents.")])]),e._v(" "),a("h2",{attrs:{id:"use-as-a-set"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#use-as-a-set"}},[e._v("#")]),e._v(' Use as a "Set"')]),e._v(" "),a("p",[e._v('The IPLD HashMap can be repurposed as a "Set": a data structure that holds only unique '),a("code",[e._v("key")]),e._v("s. Every "),a("code",[e._v("value")]),e._v(" in a "),a("code",[e._v("Set(key, value)")]),e._v(" mutation is fixed to some trivial value, such as "),a("code",[e._v("true")]),e._v(" or "),a("code",[e._v("1")]),e._v(". "),a("code",[e._v("Has(key)")]),e._v(" operations are then simply a "),a("code",[e._v("Get(key)")]),e._v(" operation that asserts that a value was returned.")]),e._v(" "),a("h2",{attrs:{id:"implementation-defaults"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#implementation-defaults"}},[e._v("#")]),e._v(" Implementation defaults")]),e._v(" "),a("p",[e._v("Implements need to ship with "),a("em",[e._v("sensible defaults")]),e._v(" and be able to create HashMaps without users requiring intimate knowledge of the algorithm and the all of the trade-offs (although such knowledge will help in their optimal use).")]),e._v(" "),a("p",[e._v("These defaults are descriptive rather than prescriptive. New implementations may opt for different defaults, while acknowledging that they will produce different graphs (and therefore CIDs) for the same data as with the defaults listed below. Users may also be provided with facilities to override these defaults to suit their use cases where these defaults do not produce optimal outcomes.")]),e._v(" "),a("h3",{attrs:{id:"hashalg"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hashalg"}},[e._v("#")]),e._v(" "),a("code",[e._v("hashAlg")])]),e._v(" "),a("ul",[a("li",[e._v("The default supported hash algorithm for writing IPLD HashMaps is the x64 form of the 128-bit "),a("a",{attrs:{href:"https://github.com/aappleby/smhasher",target:"_blank",rel:"noopener noreferrer"}},[e._v("MurmurHash3"),a("OutboundLink")],1),e._v(" (identified by the multihash name "),a("a",{attrs:{href:"https://github.com/multiformats/multicodec/blob/master/table.csv",target:"_blank",rel:"noopener noreferrer"}},[e._v("'murmur3-128'"),a("OutboundLink")],1),e._v("). Note the x86 form will produce different output so should not be confused with the x64 form. Additionally, "),a("a",{attrs:{href:"https://cimi.io/murmurhash3js-revisited/",target:"_blank",rel:"noopener noreferrer"}},[e._v("some JavaScript implementations"),a("OutboundLink")],1),e._v(" do not correctly decompose UTF-8 strings into their constituent bytes for hashing so will not produce portable results.")]),e._v(" "),a("li",[e._v("Pluggability of hash algorithms is encouraged to allow users to switch switch if their use-case has a compelling reason. Such pluggability requires the supply of an algorithm that takes Bytes and returns Bytes. Users changing the hash algorithm need to be aware that such pluggability restricts the ability of other implementations to read their data since matching hash algorithms also need to be supplied on the read-side.")])]),e._v(" "),a("h3",{attrs:{id:"bitwidth"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bitwidth"}},[e._v("#")]),e._v(" "),a("code",[e._v("bitWidth")])]),e._v(" "),a("ul",[a("li",[e._v("The default "),a("code",[e._v("bitWidth")]),e._v(" is "),a("code",[e._v("8")]),e._v(" for writing IPLD HashMaps. This value yields a "),a("code",[e._v("data")]),e._v(" length of "),a("code",[e._v("2")]),a("sup",[a("code",[e._v("8")])]),a("code",[e._v("=256")]),e._v(". "),a("code",[e._v("8")]),e._v(" is also simple in most programming languages to slice off a list of bytes since it's a simple byte-index. However, implementations should be designed to support different "),a("code",[e._v("bitWidth")]),e._v("s encountered when reading IPLD HashMaps. The minimum supported "),a("code",[e._v("bitWidth")]),e._v(" must be "),a("code",[e._v("3")]),e._v(" (for a 1-byte "),a("code",[e._v("map")]),e._v("). No maximum is specified, however implementers should be aware that interoperability problems may arise with large "),a("code",[e._v("bitWidth")]),e._v(" values. For lower-level languages, you can work out the "),a("code",[e._v("data")]),e._v(" length via the equivalent "),a("code",[e._v("1 << (bitWidth - 3)")]),e._v(".")])]),e._v(" "),a("h3",{attrs:{id:"bucketsize"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bucketsize"}},[e._v("#")]),e._v(" "),a("code",[e._v("bucketSize")])]),e._v(" "),a("ul",[a("li",[e._v("The default "),a("code",[e._v("bucketSize")]),e._v(" is "),a("code",[e._v("3")]),e._v(" for writing IPLD HashMaps. Combined with a "),a("code",[e._v("bitWidth")]),e._v(" of "),a("code",[e._v("8")]),e._v(" this yields a theoretical maximally full node (with no child nodes) of "),a("code",[e._v("256 x 3 = 768")]),e._v(" "),a("code",[e._v("key")]),e._v(" / "),a("code",[e._v("value")]),e._v(" pairs. The minimum supported "),a("code",[e._v("bucketSize")]),e._v(" should be "),a("code",[e._v("1")]),e._v(". No maximum is specified, however implementers should be aware that interoperability problems may arise with very large "),a("code",[e._v("bucketSize")]),e._v(" values.")])]),e._v(" "),a("h3",{attrs:{id:"maximum-key-size"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#maximum-key-size"}},[e._v("#")]),e._v(" Maximum key size")]),e._v(" "),a("p",[e._v("Implementations may impose a maximum key size for writing IPLD HashMaps. Reading IPLD HashMaps with keys larger than the maximum they define for reading is not defined in this specification.")]),e._v(" "),a("h3",{attrs:{id:"inline-values"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#inline-values"}},[e._v("#")]),e._v(" Inline values")]),e._v(" "),a("p",[e._v("Implementations may choose to write all values in separate blocks and store only CIDs in "),a("code",[e._v("value")]),e._v(" locations in an IPLD HashMap. Alternatively, a rough size heuristic may also be applied to make a decision regarding inline versus linked blocks. Or this decision could be left up to the user via some API choice. As storage of arbitrary kinds in "),a("code",[e._v("value")]),e._v(" locations is allowed by this specification, implementations should support this for read operations.")]),e._v(" "),a("h2",{attrs:{id:"possible-future-improvements-and-areas-for-research"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#possible-future-improvements-and-areas-for-research"}},[e._v("#")]),e._v(" Possible future improvements and areas for research")]),e._v(" "),a("h3",{attrs:{id:"maximum-depth-limitations"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#maximum-depth-limitations"}},[e._v("#")]),e._v(" Maximum depth limitations")]),e._v(" "),a("p",[e._v("One aim of IPLD collections is to support arbitrarily large data sets. This specification does not meet this requirement as there is a maximum depth imposed by the number of bits in a hash.")]),e._v(" "),a("p",[e._v("Future iterations of this specification may explore:")]),e._v(" "),a("ul",[a("li",[e._v("Default hash algorithm(s) outputting a larger number of bits (e.g. a cryptographic hash function such as SHA2-256).")]),e._v(" "),a("li",[e._v("Resetting the "),a("code",[e._v("index")]),e._v(" calculation to take bits from the start of the hash once maximum-depth is reached, allowing or theoretically infinite depth data structures.")]),e._v(" "),a("li",[e._v("Allowing flexibility in "),a("code",[e._v("bucketSize")]),e._v(" at maximum-depth nodes.")])]),e._v(" "),a("h3",{attrs:{id:"hash-algorithm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hash-algorithm"}},[e._v("#")]),e._v(" Hash algorithm")]),e._v(" "),a("ul",[a("li",[e._v("The use of MurmurHash3 x64 128-bit needs further research and modelling. There may be more appropriate default algorithms for the IPLD HashMap with more optimal characteristics (speed, randomness, suitability for a web environment, etc.).")]),e._v(" "),a("li",[e._v("There may arise a demonstrated need to encode a nonce or key in the root block to support keyed hash algorithms.")])]),e._v(" "),a("h3",{attrs:{id:"buckets"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#buckets"}},[e._v("#")]),e._v(" Buckets")]),e._v(" "),a("p",[e._v("The impact on the data structure layout imposed by the use of buckets in the IPLD HashMap needs to be researched and the costs and benefits properly quantified. It is possible that buckets may be removed from a future version of this specification depending on the results of such research.")]),e._v(" "),a("h3",{attrs:{id:"security"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#security"}},[e._v("#")]),e._v(" Security")]),e._v(" "),a("p",[e._v("As yet, there is no known hash collision attack vector against IPLD data structures. There may conceivably be use cases where user-input is able to impact "),a("code",[e._v("key")]),e._v("s and collisions against a chosen "),a("code",[e._v("hashAlg")]),e._v(" are practical. In such cases, an IPLD HashMap could be built whereby it reaches its maximum depth of "),a("code",[e._v("(digestLength x 8) / bitWidth")]),e._v(" quickly and further colliding additions cause errors and possible denial of service. The current use-cases of IPLD do not lend themselves to denial of service attacks of this kind. Further practical application and research may change this understanding and dictate the need for hash algorithms with large byte output and/or cryptographic hash algorithms without known collision flaws.")]),e._v(" "),a("h2",{attrs:{id:"appendix-filecoin-hamt-variant"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#appendix-filecoin-hamt-variant"}},[e._v("#")]),e._v(" Appendix: Filecoin HAMT Variant")]),e._v(" "),a("p",[e._v("The "),a("a",{attrs:{href:"https://filecoin-project.github.io/specs/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Filecoin Project blockchain"),a("OutboundLink")],1),e._v(" makes use of a HAMT that uses the same HAMT with buckets and CHAMP mutation semantics as outlined in this document. It encodes directly as "),a("RouterLink",{attrs:{to:"/block-layer/codecs/dag-cbor.html"}},[e._v("DAG-CBOR")]),e._v(" but uses a different block layout to the one specified here. This section documents the specific ways that the Filecoin HAMT variant differs from this specification. IPLD HashMap implementations may be able to implement a form that provides compatibility with Filecoin when requested by the user.")],1),e._v(" "),a("p",[e._v("The reference Go implementation for the Filecoin HAMT is used by the "),a("a",{attrs:{href:"https://lotu.sh/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Lotus"),a("OutboundLink")],1),e._v(" client and is available at "),a("a",{attrs:{href:"https://github.com/ipfs/go-hamt-ipld",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://github.com/ipfs/go-hamt-ipld"),a("OutboundLink")],1),e._v(".")]),e._v(" "),a("h3",{attrs:{id:"implicit-and-fixed-parameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#implicit-and-fixed-parameters"}},[e._v("#")]),e._v(" Implicit and fixed parameters")]),e._v(" "),a("p",[e._v("The Filecoin HAMT "),a("em",[e._v("does not")]),e._v(" use an explicit root block ("),a("code",[e._v("HashMapRoot")]),e._v(") to encode its parameters within the data. Instead it is expected that consumers of the data understand the parameters from a combination of the Filecoin specification and versioning of the blockchain over time. All HAMT nodes take the same form, there is no differentiation for a root node and an implementation must bring implicit parameters when decoding each node.")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("hashAlg")]),e._v(": The hash algorithm used by the Filecoin HAMT is SHA2-256.")]),e._v(" "),a("li",[a("code",[e._v("bitWidth")]),e._v(": The Filecoin HAMT fixes the bit width to "),a("code",[e._v("5")]),e._v(", meaning that each node of the HAMT can contain up to "),a("code",[e._v("2")]),a("sup",[a("code",[e._v("5")])]),e._v(" ("),a("code",[e._v("32")]),e._v(") elements containing either buckets or links to child nodes.")]),e._v(" "),a("li",[a("code",[e._v("bucketSize")]),e._v(": The Filecoin HAMT fixes the maximum length of its buckets to "),a("code",[e._v("3")]),e._v(", meaning a maximally full HAMT leaf node can contain "),a("code",[e._v("32 x 3")]),e._v(" ("),a("code",[e._v("96")]),e._v(") key/value pairs.")])]),e._v(" "),a("h3",{attrs:{id:"block-layout"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#block-layout"}},[e._v("#")]),e._v(" Block layout")]),e._v(" "),a("p",[e._v("An IPLD schema representing the Filecoin HAMT varies from the IPLD HashMap "),a("a",{attrs:{href:"#Schema"}},[e._v("schema")]),e._v(" so any implementation needing to read Filecoin HAMT blocks will need to handle its specific layout:")]),e._v(" "),a("div",{staticClass:"language-ipldsch extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ipldsch"}},[a("code",[a("span",{pre:!0,attrs:{class:"token typedef"}},[a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("type")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("HashMapNode")])]),e._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[e._v("struct")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("\n  map Bytes\n  data "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("[")]),e._v(" Element "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("]")]),e._v("\n"),a("span",{pre:!0,attrs:{class:"token representation"}},[e._v("} "),a("span",{pre:!0,attrs:{class:"token builtin"}},[e._v("representation")])]),e._v(" tuple\n\n"),a("span",{pre:!0,attrs:{class:"token typedef"}},[a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("type")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("Element")])]),e._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[e._v("union")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("|")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("&")]),e._v("HashMapNode "),a("span",{pre:!0,attrs:{class:"token string"}},[e._v('"0"')]),e._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("|")]),e._v(" Bucket "),a("span",{pre:!0,attrs:{class:"token string"}},[e._v('"1"')]),e._v("\n"),a("span",{pre:!0,attrs:{class:"token representation"}},[e._v("} "),a("span",{pre:!0,attrs:{class:"token builtin"}},[e._v("representation")])]),e._v(" keyed\n\n"),a("span",{pre:!0,attrs:{class:"token typedef"}},[a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("type")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("Bucket")])]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("[")]),e._v(" BucketEntry "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("]")]),e._v("\n\n"),a("span",{pre:!0,attrs:{class:"token typedef"}},[a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("type")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("BucketEntry")])]),e._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[e._v("struct")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("\n  key Bytes\n  value Any\n"),a("span",{pre:!0,attrs:{class:"token representation"}},[e._v("} "),a("span",{pre:!0,attrs:{class:"token builtin"}},[e._v("representation")])]),e._v(" tuple\n")])])]),a("p",[e._v("There is currently no limitation on the types available for storage as "),a("code",[e._v("value")]),e._v("s as long as they can be decoded from the bytes. In practice, the Filecoin HAMT is used to store inline objects rather than links to objects.")])])}),[],!1,null,null,null);t.default=n.exports}}]);